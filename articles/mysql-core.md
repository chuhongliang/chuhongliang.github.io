# MySQL 知识点
- [存储引擎](#存储引擎)
- [索引](#索引)
- [调优](#调优)
- [事务](#事务)
- [锁](#锁)
- [MVCC](#MVCC)
- [日志系统](#日志系统)
- [主从复制](#主从复制)
- [读写分离](#读写分离)
- [分库分表](#分库分表)
- [参考资料:淘宝数据库内核月报](http://mysql.taobao.org/monthly/2017/12/01/)
- [数据结构教学网站](https://www.cs.usfca.edu/~galles/visualization/)

# MySQL Server
- 连接器: 管理连接，验证权限
- 分析器: 词法分析，语法分析
- 优化器: （用户无法控制）
  - CBO: 基于成本的优化，应该比较广泛
  - RBO: 基于规则的优化，
- 执行器: 用来跟存储引擎直接交互。

## 存储引擎
定义：不同的数据文件在磁盘的不同组织形式。

分类：
- innodb
- myisam
- memory(Heap)


### innodb 和 myisam 的区别
- innodb 支持事务，myisam不支持
- innodb 支持外键，myisam不支持
- innodb 支持表锁和行锁，myisam只支持表锁
- innodb 在5.6版本之后支持全文索引， myisam 一直支持全文索引
- innodb 索引的叶子节点直接存放数据（聚簇索引），myisam 存放地址（非聚簇索引: 就是索引与行数据分开存储）。
  

## 索引
索引是帮助MySQL高效获取数据的排好序的数据结构

> 1. 局部性原理：数据和程序都有聚集成群的倾向，分为空间局部性和时间局部性。
> 2. 磁盘预读：内存跟磁盘在进行交互的时候要保证每次读取数据需要一个逻辑单位，而这个逻辑单位叫做页。或者叫datapage, 一般都是4K或者8K， 在进行读取的时候一般都是4K的整数倍。
innodb每次读取16KB。 [参考资料](https://blog.csdn.net/weixin_36043375/article/details/113349291)

### 索引的数据结构：
- hash: memory 使用hash索引，innodb支持自适应hash索引。单行查询快，不支持范围查询
- 树: 
  - 二叉树：存在单边增长的问题
  - 红黑树(平衡树)：
  - B 树(B-树)：
    - 叶子节点具有相同的深度
    - 叶子节点的指针为空
    - 节点中的数据索引从左到右递增排列
  - B+ 树: 
    - 多叉树
    - 节点有序
    - 每一个节点可以存储多条记录
    - 非叶子节点不存储data，只存储索引，可以放更多的索引
    - 叶子节点包含所有的索引字段
    - 叶子节点用指针连指顺序访问，提高区间访问的性能


### mysql的索引一般有几层？
一般情况下，3到4层就足以支撑千万级别的表的查询。

### 创建索引的字段是长了好还是短了好？
短了好，原因是在层数不变的情况下可以存储更多的数据量。

### 创建表的时候是用代理主键还是用自然主键？
- 代理主键：跟业务无关字段（id）
- 自然主键：跟业务有关的字段（phonenumber）
能使用代理主键尽可能的使用代理主键。

### 主键设置好之后，要不要自增？
在满足业务的情况下尽可能自增，不自增会增加索引的维护成本。

### 在分布式应用场中，自增id还适用吗？
不适用，雪花算法snowflake, 自定义id生成器。

### 聚簇索引和非聚簇索引
- 聚簇索引: 数据跟索引聚集存储的　innodb的主键索引
  - innodb的主键索引就是聚簇索引，必须要包含一个主键列的：key (直接存放数据)
  - 如果在创建表的时候制定了主键，那么key就是主键，如果没有主键，那么key就是唯一键。如果唯一键也没有，那么key就是6字节的rowID。
- 非聚簇索索引：数据跟索引不是聚集存储的
  - myisam 的所有索引
  - innodb 的二级索引(辅助索引，普通索引) (辅助索引，普通索引的叶子节点存放主键)(一致性和节省存储空间)

### 回表查询
InnoDB普通索引的叶子节点存储主键值，普通索引因为无法直接定位行记录，其查询过程在通常情况下是需要扫描两遍索引树。

### 最左匹配原则
在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。

### 索引下推
简称ICP，在Mysql5.6的版本上推出，用于优化查询。

其实就是对索引失效的进一步修复，属于最左前缀索引原则的一个意外情况。

索引下推触发的条件
- 查询条件是符合索引
- 失效条件的字段在索引覆盖的范围内
- 失效条件是可以通过数据进行比较的简单对比


### 索引覆盖
索引覆盖是一种避免回表查询的优化策略。具体的做法就是将要查询的数据作为索引列建立普通索引（可以是单列索引，也可以一个索引语句定义所有要查询的列，即联合索引），这样的话就可以直接返回索引中的的数据，不需要再通过聚集索引去定位行记录，避免了回表的情况发生。

#### 注意事项
如果一个索引覆盖（包含）了所有需要查询的字段的值，这个索引就是覆盖索引。因为索引中已经包含了要查询的字段的值，因此查询的时候直接返回索引中的字段值就可以了，不需要再到表中查询，避免了对主键索引的二次查询，也就提高了查询的效率。

要注意的是，不是所有类型的索引都可以成为覆盖索引的。因为覆盖索引必须要存储索引的列值，而哈希索引、空间索引和全文索引等都不存储索引列值，索引MySQL只能使用B-Tree索引做覆盖索引。

另外，当发起一个被索引覆盖的查询（索引覆盖查询）时，在explain（执行计划）的Extra列可以看到【Using Index】的信息。

#### 覆盖索引的优点

- 1.索引条目通常远小于数据行的大小，因为覆盖索引只需要读取索引，极大地减少了数据的访问量。

- 2.索引是按照列值顺序存储的，对于IO密集的范围查找会比随机从磁盘读取每一行数据的IO小很多。

- 3.一些存储引擎比如MyISAM在内存中只缓存索引，数据则依赖操作系统来缓存，因此要访问数据的话需要一次系统调用，使用覆盖索引则避免了这一点。

- 4.由于InnoDB的聚簇索引，覆盖索引对InnoDB引擎下的数据库表特别有用。因为InnoDB的二级索引在叶子节点中保存了行的主键值，如果二级索引能够覆盖查询，就避免了对主键索引的二次查询。

### 索引失效
- 1、对列使用函数，该列的索引将不起作用。
  - 如：substring(字段名,1,2)='xxx'；

- 2、对列进行运算(+，-，*，/，! 等)，该列的索引将不起作用。
  - 如：
  ```sql
  select * from test where id-1=9;//错误的写法；

  select * from test where id=10; //正确的写法 ；
  ```

- 3、某些情况下的LIKE操作，该列的索引将不起作用。

  - 如：字段名 LIKE CONCAT('%', '2014 - 08 - 13', '%') ；

- 4、某些情况使用反向操作，该列的索引将不起作用。

  - 如：字段名 <> 2；

- 5、在WHERE中使用OR时，有一个列没有索引，那么其它列的索引将不起作用。

- 6、隐式转换导致索引失效.这一点应当引起重视.也是开发中经常会犯的错误。

  - 由于表的字段t_number定义为varchar2(20),但在查询时把该字段作为number类型以where条件传给Oracle,这样会导致索引失效。

  - 如: 
  ```sql
  select * from test where t_number=13333333333;  //错误的写法；

  select * from test where t_number='13333333333'; //正确的写法；
  ```
- 7、使用not in ,not exist等语句时。

- 8、当变量采用的是times变量，而表的字段采用的是date变量时.或相反情况。

- 9、当B-tree索引 is null不会失效,使用is not null时,会失效,位图索引 is null,is not null 都会失效。

- 10、联合索引 is not null 只要在建立的索引列（不分先后）都会失效。

  - in null时 必须要和建立索引第一列一起使用,当建立索引第一位置条件是is null 时,其他建立索引的列可以是is null（但必须在所有列 都满足is null的时候）,或者 = 一个值；

  - 当建立索引的第一位置是 = 一个值时,其他索引列可以是任何情况（包括is null  = 一个值）,以上两种情况索引都会失效,其他情况不会失效。

## 调优
参考资料：https://www.cnblogs.com/igoodful/p/9360863.html

### 基础规范
1. 不在数据库做运算：cpu计算务必移至业务层
2. 控制单表数据量：单表记录控制在1000w
3. 控制列数量：字段数控制在20以内
4. 平衡范式与冗余：为提高效率牺牲范式设计，冗余数据
5. 拒绝3B：拒绝大sql，大事物，大批量
6. 表存储引擎必须使用InnoDB（通用，无乱码风险，汉字3字节，英文1字节）
7. 表字符集默认使用utf8，必要时候使用utf8mb4（utf8mb4是utf8的超集，有存储4字节例如表情符号时，使用它）
8. 禁止使用存储过程，视图，触发器，Event
   - 对数据库性能影响较大，互联网业务，能让站点层和服务层干的事情，不要交到数据库层
   - 调试，排错，迁移都比较困难，扩展性较差
9. 禁止在数据库中存储大文件，例如照片，可以将大文件存储在对象存储系统，数据库中存储路径
10. 禁止在线上环境做数据库压力测试
11. 测试，开发，线上数据库环境必须隔离

### 命名规范
1. 库名，表名，列名必须用小写，采用下划线分隔
   - 解读：abc，Abc，ABC都是给自己埋坑
2. 库名，表名，列名必须见名知义，长度不要超过32字符
   - 解读：tmp，wushan谁TM知道这些库是干嘛的....
3. 库备份必须以bak为前缀，以日期为后缀
4. 从库必须以-s为后缀
5. 备库必须以-ss为后缀

### 表设计规范
1. 单实例表个数必须控制在2000个以内
2. 单表分表个数必须控制在1024个以内
3. 表必须有主键，推荐使用UNSIGNED整数为主键
  - 潜在坑：删除无主键的表，如果是row模式的主从架构，从库会挂住
4. 禁止使用外键，如果要保证完整性，应由应用程式实现
  - 解读：外键使得表之间相互耦合，影响update/delete等SQL性能，有可能造成死锁，高并发情况下容易成为数据库瓶颈
5. 建议将大字段，访问频度低的字段拆分到单独的表中存储，分离冷热数据


### 列设计规范
1. 根据业务区分使用tinyint/smallint/mediumint/int/bigint
    ```sql
    tinyint(1Byte)     -128 - 127
    smallint(2Byte)    -32768 - 32767
    mediumint(3Byte)   -8388608 - 8388607
    int(4Byte)         -2147483648 - 2147483647
    bigint(8Byte)      -9223372036854775808 - 9223372036854775807
    ```

2. 根据业务区分使用char/varchar
   - 字段长度固定，或者长度近似的业务场景，适合使用char，能够减少碎片，查询性能高
   - 字段长度相差较大，或者更新较少的业务场景，适合使用varchar，能够减少空间

3. 根据业务区分使用datetime/timestamp
   - 解读：前者占用5个字节，后者占用4个字节，存储年使用YEAR，存储日期使用DATE，存储时间使用datetime

4. 必须把字段定义为NOT NULL并设默认值
   - NULL的列使用索引，索引统计，值都更加复杂，MySQL更难优化
   - NULL需要更多的存储空间
   - NULL只能采用IS NULL或者IS NOT NULL，而在=/!=/in/not in时有大坑

5. 使用varchar(20)存储手机号，不要使用整数
   - 牵扯到国家代号，可能出现+/-/()等字符，例如+86
   - 手机号不会用来做数学运算
   - varchar可以模糊查询，例如like ‘138%’

6. 使用INT UNSIGNED存储IPv4，不要用char(15)

7. 使用TINYINT来代替ENUM
    - 解读：ENUM增加新值要进行DDL操作

8. 少用text/blob
     - varchar的性能会比text高很多
     - 实在避免不了blob，请拆表

9. 不在数据库里存图片：是否需要解释？


### 索引规范
1. 唯一索引使用uniq_[字段名]来命名
2. 非唯一索引使用idx_[字段名]来命名
3. 单张表索引数量建议控制在5个以内
    - 互联网高并发业务，太多索引会影响写性能
    - 生成执行计划时，如果索引太多，会降低性能，并可能导致MySQL选择不到最优索引
    - 异常复杂的查询需求，可以选择ES等更为适合的方式存储

4. 组合索引字段数不建议超过5个
    - 解读：如果5个字段还不能极大缩小row范围，八成是设计有问题

5. 不建议在频繁更新的字段上建立索引
6. 非必要不要进行JOIN查询，如果要进行JOIN查询，被JOIN的字段必须类型相同，并建立索引
    - 解读：踩过因为JOIN字段类型不一致，而导致全表扫描的坑么？
7. 理解组合索引最左前缀原则，避免重复建设索引，如果建立了(a,b,c)，相当于建立了(a), (a,b),(a,b,c)

8. 不在索引做列运算

9. 不用外键
    - 请由程序保证约束

10. innodb主键推荐使用自增列
    - 主键建立聚簇索引
    - 主键不应该被修改
    - 字符串不应该做主键
    - 如果不指定主键，innodb会使用唯一且非空值索引代替

### SQL规范
1. 禁止使用select *，只获取必要字段
    - select *会增加cpu/io/内存/带宽的消耗
2. 隐式类型转换会使索引失效，导致全表扫描
3. 禁止在where条件列使用函数或者表达式
    - 导致不能命中索引，全表扫描
4. 禁止负向查询以及%开头的模糊查询
   - 导致不能命中索引，全表扫描
5. 禁止大表JOIN和子查询
6. 同一个字段上的OR必须改写成IN，IN的值必须少于50个
7. 应用程序必须捕获SQL异常
   - 方便定位线上问题
8. 避免使用trig/func
   - 触发器、函数不用
   - 客户端程序取而代之
9. 简单的事务
   - 事务时间尽可能短
10. sql语句尽可能简单
   - 一条sql只能在一个cpu运算
   - 大语句拆小语句，减少锁时间
   - 一条大sql可以堵死整个库

11. OR改写为UNION
   - mysql的索引合并很弱智
   ```sql
    　 select id from t where phone = ’159′ or name = ‘john’;
        =>
       select id from t where phone=’159′ union select id from t where name=’jonh’;
   ```

12. 慎用count(*)

13. limit高效分页 limit越大，效率越低 
    ```sql
      select id from t limit 10000, 10; 
      => 
      select id from t where id > 10000 limit 10; 
    ```
14. 使用union all替代union 
    - union有去重开销 
15. 少用连接join

16. 使用group by 
    - 分组； 
    - 自动排序；

17. 请使用同类型比较 
18. 使用load data导数据 
    load data 比 insert 快约20倍； 

19. 打散批量更新 

20. 新能分析工具 
    - show profile; 
    - mysqlsla;
    - mysqldumpslow; 
    - explain; 
    - show slow log; 
    - show processlist; 
    - show query_response_time(percona);







## 事务
- 原子性：undo log 　保存的是跟执行操作相反的操作，用于回滚。保证原子性，参与部分的mvcc(多版本并发控制)操作。
- 一致性：
- 隔离性：用锁来保证 
  - 读未提交：会触发脏读，幻读，不可重复读。
  - 读已提交：会触发幻读，不可重复读。   读取的是最新的一致性的快照版本。
  - 可重复读：会触发幻读　(默认级别)　　 读取的是事务开启之前的版本。(读的快照)
  - 串行化(序列化)：
  - 隔离级别越低，效率越高，越不安全；隔离级别越高，效率越低，越安全。
- 持久性：redo log 　为了保证 crash safe; 如果发生异常情况，就算数据没有持久化成功，只要日志持久化成功了，依然可以进行恢复。

### 更新丢失（lost update）
当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题，最后的更新覆盖了由其他事务所做的更新。

### 脏读（dirty reads）(读取了前一事务 未提交 的数据)
一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。

### 不可重复读（non-repeatable reads）(读取了前一事务提交 的数据)
一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变或某些记录已经被删除了！这种现象就是“不可重复读”。

### 幻读（phantom reads）
一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象称为“幻读”。

### 事务隔离级别
并发事务处理带来的问题中，“更新丢失”，通常是可以避免的，需要应用程序对要更新的数据加必要的锁来解决。

“脏读”，“不可重复读”和“幻读”， 其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。

数据库实现事务隔离的方式，基本可以分为两种：
- 在读取数据前，对其加锁，阻止其他事务对数据进行修改
- 不加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照，这种方式叫做数据多版本并发控制。

数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上“串行化”进行，这显然与“并发”是矛盾的。为了解决“隔离”与“并发”的矛盾，ISO/ANSI SQL92 定义了 4 个事务隔离级别，MySQL 实现了这四种级别，应用可以根据自己的业务逻辑要求，选择合适的隔离级别来平衡“隔离”与“并发”的矛盾。



## 锁
锁是计算机协调多个进程或线程并发访问某一资源的机制。锁保证数据并发访问的一致性、有效性；锁冲突也是影响数据库并发访问性能的一个重要因素。锁是Mysql在服务器层和存储引擎层的的并发控制。

加锁是消耗资源的，锁的各种操作，包括获得锁、检测锁是否是否已解除、释放锁等。

### 共享锁与排他锁
- 共享锁（读锁）：其他事务可以读，但不能写。
- 排他锁（写锁）：其他事务不能读取，也不能写。

### 粒度锁
MySQL 不同的存储引擎支持不同的锁机制，所有的存储引擎都以自己的方式显现了锁机制，服务器层完全不了解存储引擎中的锁实现：
- 表级锁: MyISAM 和 MEMORY 存储引擎采用的是表级锁（table-level locking）
- 页面锁: BDB 存储引擎采用的是页面锁（page-level locking），但也支持表级锁
- 行级锁: InnoDB 存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。

默认情况下，表锁和行锁都是自动获得的， 不需要额外的命令。

但是在有的情况下， 用户需要明确地进行锁表或者进行事务的控制， 以便确保整个事务的完整性，这样就需要使用事务控制和锁定语句来完成。

### 不同粒度锁的比较：
1. 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。
    - 这些存储引擎通过总是一次性同时获取所有需要的锁以及总是按相同的顺序获取表锁来避免死锁。
    - 表级锁更适合于以查询为主，并发用户少，只有少量按索引条件更新数据的应用，如Web 应用
2. 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
    - 最大程度的支持并发，同时也带来了最大的锁开销。
    - 在 InnoDB 中，除单个 SQL 组成的事务外，锁是逐步获得的，这就决定了在 InnoDB 中发生死锁是可能的。
    - 行级锁只在存储引擎层实现，而Mysql服务器层没有实现。 行级锁更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统
3. 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。


### MyISAM 表锁
MyISAM表级锁模式：
- 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；
- 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作；



### MyISAM 的锁调度
#### MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。

#### 默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。
这也正是 MyISAM 表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。

同时，一些需要长时间运行的查询操作，也会使写线程“饿死” ，应用中应尽量避免出现长时间运行的查询操作
 - 在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解” ，使每一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行。

MyISAM设置改变读锁和写锁的优先级：
- 通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。
- 通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低。
- 通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。
- 给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。

### MyISAM加表锁方法：
MyISAM 在执行查询语句（SELECT）前，会自动给涉及的表加读锁，在执行更新操作
（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。

在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因。

### MyISAM并发插入（concurrent inserts）
myisam 表的读和写是串行的，但这是就总体而言的。在一定条件下，myisam 表也支持查询和插入操作的并发进行。
myisam 存储引擎有一个系统变量 concurrent_insert , 专门用以控制其并发插入的行为，其值分别可以为0,1,2。

- 当 concurrent_insert 设置为 0 时，不允许并发插入。
- 当 concurrent_insert 设置为 1 时，如果 myisam 表中没有空洞（即表的中间没有被删除的行），myisam 允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是 MySQL 的默认设置。
- 当 concurrent_insert 设置为 2 时，无论 myisam 表中有没有空洞，都允许在表尾并发插入记录。

查询表级锁争用情况：

可以通过检查 table_locks_waited 和 table_locks_immediate 状态变量来分析系统上的表锁的争夺，如果 Table_locks_waited 的值比较高，则说明存在着较严重的表级锁争用情况：
```sql
mysql> SHOW STATUS LIKE 'Table%';
+-----------------------+---------+
| Variable_name | Value |
+-----------------------+---------+
| Table_locks_immediate | 1151552 |
| Table_locks_waited | 15324 |
+-----------------------+---------+
```


### InnoDB 锁
InnoDB 实现了以下两种类型的行锁：
- 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。
- 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。

为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：
- 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。
- 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。

```sql
+-----------------------+---------+-----------------------+---------+------------+
| 　　    锁名称　　  | 共享锁(S) |  排他锁(X) |  意向共享锁(IS) |  意向排他锁(IX) |
+-----------------------+---------+-----------------------+---------+------------+
| 排他锁(X)          |    冲突   |     冲突   |       冲突     |      冲突     |
| 意向排他锁(IX)     |    冲突   |     兼容   |       冲突     |      兼容     |
| 共享锁(S)　        |    冲突   |     冲突   |       兼容     |      兼容     |
| 意向共享锁(IS)     |    冲突   |     兼容   |       兼容     |      兼容     |
+-----------------------+---------+-----------------------+---------+------------+
```
如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务； 反之， 如果两者不兼容，该事务就要等待锁释放

### InnoDB加锁方法：
- 意向锁是 InnoDB 自动加的， 不需用户干预。
- 对于 UPDATE、 DELETE 和 INSERT 语句， InnoDB会自动给涉及数据集加排他锁（X)；
- 对于普通 SELECT 语句，InnoDB 不会加任何锁；
- 事务可以通过以下语句显式给记录集加共享锁或排他锁：
    - 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。
    - 排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁


#### 隐式锁定：
- InnoDB在事务执行过程中，使用两阶段锁协议：
- 随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁；
- 锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。

#### 显式锁定：
```sql
select ... lock in share mode // 共享锁 
select ... for update // 排他锁  
```

select for update：
- 作用: 在执行这个 select 查询语句的时候，会将对应的索引访问条目进行上排他锁（X 锁），也就是说这个语句对应的锁就相当于update带来的效果。
- 使用场景：为了让自己查到的数据确保是最新数据，并且查到后的数据只允许自己来修改的时候，需要用到 for update 子句。
- 性能：相当于一个 update 语句。在业务繁忙的情况下，如果事务没有及时的commit或者rollback 可能会造成其他事务长时间的等待，从而影响数据库的并发使用效率。

select lock in share mode ：
- 作用: in share mode 子句的作用就是将查找到的数据加上一个 share 锁，这个就是表示其他的事务只能对这些数据进行简单的select 操作，并不能够进行 DML 操作。
- 使用场景：为了确保自己查到的数据没有被其他的事务正在修改，也就是说确保查到的数据是最新的数据，并且不允许其他人来修改数据。但是自己不一定能够修改数据，因为有可能其他的事务也对这些数据 使用了 in share mode 的方式上了 S 锁。
- 性能：是一个给查找的数据上一个共享锁（S 锁）的功能，它允许其他的事务也对该数据上S锁，但是不能够允许对该数据进行修改。如果不及时的commit 或者rollback 也可能会造成大量的事务等待。


for update 和 lock in share mode 的区别：
- for update 上的是排他锁（X 锁），一旦一个事务获取了这个锁，其他的事务是没法在这些数据上执行 for update ；
- lock in share mode 是共享锁，多个事务可以同时的对相同数据执行 lock in share mode。


#### InnoDB 行锁实现方式：
InnoDB 行锁是通过给索引项加锁来实现的，如果么有索引，innodb 将通过隐藏的聚簇索引来对记录加锁。innodb 行锁分为 3 种情形：
- record lock： 对索引项加锁
- gap lock： 对索引项之间的“间隙”、第一条记录前的“间隙”或最后一条记录的“间隙”加锁。
- next-key lock： 前两种的结合，对记录及其前面的间隙加锁。

InnoDB 这种行锁实现特点意味着：如果不通过索引条件检索数据，那么 innodb 将对表中的所有记录加锁，实际效果和表锁一样！

在实际应用中，要特别注意 innodb 行锁的这一特性，否则可能导致大量的锁冲突，从而影响并发性能。
- 在不通过索引条件查询的时候，InnoDB确实使用的是表锁，而不是行锁。
- 不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁。
- 只有执行计划真正使用了索引，才能使用行锁：即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。因此，在分析锁冲突时别忘了检查 SQL 的执行计划（可以通过 explain 检查 SQL 的执行计划），以确认是否真正使用了索引。
- 由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然多个session是访问不同行的记录， 但是如果是使用相同的索引键， 是会出现锁冲突的（后使用这些索引的session需要等待先使用索引的session释放锁后，才能获取锁）。 应用设计的时候要注意这一点。


### InnoDB的间隙锁（Next-Key锁）
当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。

很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。

#### InnoDB使用间隙锁的目的：
- 防止幻读，以满足相关隔离级别的要求；
- 满足恢复和复制的需要：

#### 恢复和复制的需要，对 innodb 锁机制的影响
MySQL 通过 binlog 记录执行成功的 insert、update 、delete 等更新数据的 sql 语句，并由此实现 MySQL 数据库的恢复和主从复制。MySQL 的恢复机制（复制其实就是在 Slave Mysql 不断做基于 BINLOG 的恢复）有以下特点：
1. MySQL 的恢复是 SQL 语句级的，也就是重新执行 BINLOG 中的 SQL 语句。
2. MySQL 的 Binlog 是按照事务提交的先后顺序记录的， 恢复也是按这个顺序进行的。

MySQL 5.6 支持 3 种 日志格式，即基于语句的日志格式 sbl，基于行的日志格式 rbl 和混合格式。它还支持 4 种复制模式：

- 基于 sql 语句的复制 sbr：这也是 MySQL 最早支持的复制模式。
- 基于 行数据的复制 rbr： 这是 MySQL5.1 以后喀什支持的复制模式，主要优点是支持对非安全 sql 的复制模式。
- 混合复制模式：对安全的 sql 语句采用基于 sql 语句的复制模式，对于非安全的 sql 语句采用局于行的复制模式。
- 使用全局事务id（gtids）的复制：主要是解决主从自动同步一致的问题。

对基于语句日志格式（sbl）的恢复和复制而言，由于 MySQL 的 binlog 是按照事务提交的先后顺序记录的，因此要正确恢复或复制数据，就必须满足：

在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读。

这已经超过了“可重复读”隔离级别的要求，实际上是要求事务要串行化。这也是许多情况下，innodb 要用 next-key 锁的原因。


### 什么时候使用表锁
对于 innodb 表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们选择 innodb 表的理由，但在个别特殊任务中，也可以考虑使用表级锁：
1. 事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁
2. 事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定多个表，从而避免死锁，减少数据库因事务回滚带来的开销。

当然，应用中这两种事务不能太多，否则，就应该考虑使用 myisam 表了。
#### 在 innodb 下，使用表锁要注意以下两点：
1. 使用 lock tables 虽然可以给 innodb 加表级锁，但必须说明的是，表锁不是由 innodb 存储引擎管理的，而是由其上一层———— MySQL server 负责的，仅当 autocommit=0、innodb_table_locks=1(默认设置)时，innodb 层才知道 MySQL 加的表锁，MySQL server 也才能够感知 innodb 加的行锁，这种情况下，innodb 才能自动识别涉及到的锁。
2. 在用 lock_tables 对 innodb 表加锁时要注意，要将 autocommit 设为 0，否则 MySQL 不会给表加锁；事务结束前，不要用 unlock tables 释放表锁，因为 unlock tables 会隐含的提交事务；commit 或 rollback 并不能释放用 lock tables 加的表锁，必须用 unlock tables 释放表锁


### 死锁
MyISAM表锁是deadlock free的，这是因为MyISAM总是一次获得所需的全部锁，要么全部满足，要么等待，因此不会出现死锁。但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，这就决定了在InnoDB中发生死锁是可能的。

发生死锁后，InnoDB一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB并不能完全自动检测到死锁，这需要通过设置锁等待超时参数 innodb_lock_wait_timeout来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占用大量计算机资源，造成严重性能问题，甚至拖跨数据库。我们通过设置合适的锁等待超时阈值，可以避免这种情况发生。


#### 避免死锁的常用方法
1. 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会。在下面的例子中，由于两个session访问两个表的顺序不同，发生死锁的机会就非常高！但如果以相同的顺序来访问，死锁就可以避免。
2. 在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。
3. 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。
4. 前面讲过，在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT...FOR UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可避免问题。
5. 当隔离级别为READ COMMITTED时，如果两个线程都先执行SELECT...FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第1个线程提交后，第2个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁！这时如果有第3个线程又来申请排他锁，也会出现死锁。

如果出现死锁，可以用SHOW INNODB STATUS命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的SQL语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。

#### 一些优化锁性能的建议
- 尽量使用较低的隔离级别；
- 精心设计索引， 并尽量使用索引访问数据， 使加锁更精确， 从而减少锁冲突的机会
- 选择合理的事务大小，小事务发生锁冲突的几率也更小
- 给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁
- 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会
- 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响
- 不要申请超过实际需要的锁级别
- 除非必须，查询时不要显示加锁。 MySQL的MVCC可以实现事务中的查询不用加锁，优化事务性能；MVCC只在COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作
- 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能

### 乐观锁、悲观锁
无论是悲观锁还是乐观锁，他们本质上不是数据库中具体的锁概念，而是我们定义出来，用来描述两种类别的锁的思想。所以有了设计的分类，我们就可以通过这个分类去对数据库中具体的锁进行分门别类；

不过数据库中的乐观锁更倾向叫乐观并发控制（OCC），悲观锁叫悲观并发控制（PCC），还有区别于乐观悲观锁的一种控制叫MVCC，多版本并发控制

也不要把乐观锁和悲观锁与数据库中的行锁，表锁，排他锁，共享锁混为一谈，他们并不是一个维度的东西；前者是一个锁思想，可以将后者根据是否进行趋近于乐观或悲观锁的思想进行分类
#### 乐观锁(Optimistic Lock)：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁不能解决脏读的问题。
乐观锁, 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。


#### 悲观锁(Pessimistic Lock)：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。
悲观锁，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。


参考资料:
- [正确的理解MySQL的乐观锁，悲观锁与MVCC](https://blog.csdn.net/SnailMann/article/details/88388829)




## MVCC 机制
>全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。

MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读

### 什么是当前读和快照读？
- 当前读：像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁
- 快照读：像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本
说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现


### 当前读，快照读和MVCC的关系
- 准确的说，MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念。仅仅是一个理想概念
- 而在MySQL中，实现这么一个MVCC理想概念，我们就需要MySQL提供具体的功能去实现它，而快照读就是MySQL为我们实现MVCC理想模型的其中一个具体非阻塞读功能。而相对而言，当前读就是悲观锁的具体功能实现
- 要说的再细致一些，快照读本身也是一个抽象概念，再深入研究。MVCC模型在MySQL中的具体实现则是由 3个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理


### MVCC能解决什么问题，好处是？

数据库并发场景有三种，分别为：
- 读-读：不存在任何问题，也不需要并发控制
- 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读
- 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失

### MVCC带来的好处是？
多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题
- 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能 
- 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题

小结：

总之，MVCC就是因为大牛们，不满意只让数据库采用悲观锁这样性能不佳的形式去解决读-写冲突问题，而提出的解决方案，所以在数据库中，因为有了MVCC，所以我们可以形成两个组合：

- MVCC + 悲观锁
    - MVCC解决读写冲突，悲观锁解决写写冲突
- MVCC + 乐观锁
    - MVCC解决读写冲突，乐观锁解决写写冲突

这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题

### MVCC的实现原理
MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念

#### 隐式字段：
每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段
- DB_TRX_ID：6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID
- DB_ROLL_PTR：7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里）
- DB_ROW_ID：6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引
- 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了

DB_ROW_ID是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID是当前操作该记录的事务ID,而DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本

#### undo日志
undo log主要分为两种：

- insert undo log：代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃
- update undo log：事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除
>purge 
- 从前面的分析可以看出，为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的deleted_bit，并不真正将过时的记录删除。
- 为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。

对MVCC有帮助的实质是update undo log ，undo log实际上就是存在rollback segment中旧记录链.

不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录.

### Read View(读视图)
Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)

所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。

Read View遵循一个可见性算法，主要是将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本


### RC,RR级别下的InnoDB快照读有什么不同？

正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同
- 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；

- 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见

- 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因

总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。 

参考资料:
- [MySQL · 引擎特性 · InnoDB 事务系统](http://mysql.taobao.org/monthly/2017/12/01/)
- [正确的理解MySQL的MVCC及实现原理](htts://blog.csdn.net/SnailMann/article/details/94724197)









## 日志系统
### MySQL中有以下日志文件，分别是：
1. 重做日志（redo log）
2. 回滚日志（undo log）
3. 二进制日志（binlog）
4. 错误日志（errorlog）
5. 慢查询日志（slow query log）
6. 一般查询日志（general log）
7. 中继日志（relay log）。

#### 其中重做日志和回滚日志与事务操作息息相关，二进制日志也与事务操作有一定的关系，这三种日志，对理解MySQL中的事务操作有着重要的意义。

### 重做日志（redo log）
和大多数关系型数据库一样，InnoDB 记录了对数据文件的物理更改，并保证总是日志先行，也就是所谓的 WAL，即在持久化数据文件前，保证之前的 redo 日志已经写到磁盘。由于 redo log 是顺序整块写入，所以性能要更好。

重做日志两部分组成：一是内存中的重做日志缓冲(redo log buffer)，是易失的；二是重做日志文件(redo log file)，是持久的。redo log 记录事务操作的变化，记录的是数据修改之后的值，不管事务是否提交都会记录下来。

#### 作用：
- 确保事务的持久性。redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。

#### 写入过程
在一条语句进行执行的时候，InnoDB 引擎会把新记录写到 redo log 日志中，然后更新内存，更新完成后就算是语句执行完了，然后在空闲的时候或者是按照设定的更新策略将 redo log 中的内容更新到磁盘中。

更详细的步骤，需要了解两个关键词：checkpoint 和 LSN(Log Sequence Number)，前者检查点简单来说就是把脏页刷到磁盘的时间点，这个时间点之前的数据都已经保存到了持久存储。而 LSN 是 InnoDB 使用的一个版本标记的计数，它是一个单调递增的值。数据页和 redo log 都有各自的 LSN。每次把 redo log 中的内容写入到实际的数据页之后，就会把 LSN 也同步过去。如果发生了宕机，我们可以根据数据页中的 LSN 值和 redo log 中 LSN 的值判断需要恢复的 redo log 的位置和大小。redo log 同样也有自己的缓存，所以也涉及到刷盘策略，是通过innodb_flush_log_at_trx_commit这个参数控制的。

当对应事务的脏页写入到磁盘之后，redo log 的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。

#### 存储结构
这一块应该就没必要深入了，redo log 的存储都是以块(block)为单位进行存储的，每个块的大小为 512 字节。同磁盘扇区大小一致，可以保证块的写入是原子操作。

另外 redo log 占用的空间是固定的，会循环写入。文件大小由innodb_log_file_size参数控制。


#### 内容：
- 物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。

#### 什么时候产生：
- 事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo log文件中。

#### 什么时候释放：
- 当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。

#### 对应的物理文件：
- 默认情况下，对应的物理文件位于数据库的data目录下的ib_logfile1&ib_logfile2
- innodb_log_group_home_dir 指定日志文件组所在的路径，默认./ ，表示在数据库的数据目录下。
- innodb_log_files_in_group 指定重做日志文件组中文件的数量，默认2


#### 关于文件的大小和数量，由以下两个参数配置：
- innodb_log_file_size 重做日志文件的大小。
- innodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认1

#### 其他：
很重要一点，redo log是什么时候写盘的？前面说了是在事物开始之后逐步写盘的。

之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，原因就是，重做日志有一个缓存区Innodb_log_buffer，Innodb_log_buffer的默认大小为8M(这里设置的16M),Innodb存储引擎先将重做日志写入innodb_log_buffer中。

然后会通过以下三种方式将innodb日志缓冲区的日志刷新到磁盘
- Master Thread 每秒一次执行刷新Innodb_log_buffer到重做日志文件。
- 每个事务提交时会将重做日志刷新到重做日志文件。
- 当重做日志缓存可用空间 少于一半时，重做日志缓存被刷新到重做日志文件
- 由此可以看出，重做日志通过不止一种方式写入到磁盘，尤其是对于第一种方式，Innodb_log_buffer到重做日志文件是Master Thread线程的定时任务。
- 因此重做日志的写盘，并不一定是随着事务的提交才写入重做日志文件的，而是随着事务的开始，逐步开始的。

另外引用《MySQL技术内幕 Innodb 存储引擎》（page37）上的原话：
- 即使某个事务还没有提交，Innodb存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。
- 这一点是必须要知道的，因为这可以很好地解释再大的事务的提交（commit）的时间也是很短暂的。


### 回滚日志（undo log）
undo log 有两个作用：提供回滚和多版本并发控制下的读(MVCC)，也即非锁定读

在数据修改的时候，不仅记录了redo，还记录了相对应的 undo，如果因为某些原因导致事务失败或回滚了，可以借助该 undo 进行回滚。

undo log 和 redo log 记录物理日志不一样，它是逻辑日志。可以认为当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，反之亦然，当 update 一条记录时，它记录一条对应相反的 update 记录。

有时候应用到行版本控制的时候，也是通过 undo log 来实现的：当读取的某一行被其他事务锁定时，它可以从 undo log 中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。

undo log 是采用段(segment)的方式来记录的，每个 undo 操作在记录的时候占用一个 undo log segment。

另外，undo log 也会产生 redo log，因为 undo log 也要实现持久性保护。

当事务提交的时候，InnoDB 不会立即删除 undo log，因为后续还可能会用到 undo log，如隔离级别为 repeatable read 时，事务读取的都是开启事务时的最新提交行版本，只要该事务不结束，该行版本就不能删除，即 undo log 不能删除。

当事务提交之后，undo log 并不能立马被删除，而是放入待清理的链表，由 purge 线程判断是否有其他事务在使用 undo 段中表的上一个事务之前的版本信息，决定是否可以清理 undo log 的日志空间。

在 MySQL 5.7 之前，undo log 存储在共享表空间中，因此有可能大大增加表空间的占用，5.7 之后可以通过配置选择存储在独立的表空间中。

#### 作用：
- 保证数据的原子性，保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读

#### 内容：
- 逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。

#### 什么时候产生：
- 事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性

#### 什么时候释放：
- 当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间

#### 对应的物理文件：
- MySQL5.6之前，undo表空间位于共享表空间的回滚段中，共享表空间的默认的名称是ibdata，位于数据文件目录中。
- MySQL5.6之后，undo表空间可以配置成独立的文件，但是提前需要在配置文件中配置，完成数据库初始化后生效且不可改变undo log文件的个数
- 如果初始化数据库之前没有进行相关配置，那么就无法配置成独立的表空间了。

#### 关于MySQL5.7之后的独立undo 表空间配置参数如下：
- innodb_undo_directory = /data/undospace/ –undo独立表空间的存放目录 innodb_undo_logs = 128 –回滚段为128KB innodb_undo_tablespaces = 4 –指定有4个undo log文件
- 如果undo使用的共享表空间，这个共享表空间中又不仅仅是存储了undo的信息，共享表空间的默认为与MySQL的数据目录下面，其属性由参数innodb_data_file_path配置。

#### 其他：
- undo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。
- 默认情况下undo文件是保持在共享表空间的，也即ibdatafile文件中，当数据库中发生一些大的事务性操作的时候，要生成大量的undo信息，全部保存在共享表空间中的。
- 因此共享表空间可能会变的很大，默认情况下，也就是undo 日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的。
- 因此，mysql5.7之后的“独立undo 表空间”的配置就显得很有必要了。

### undo log 和 redo log
undo log 和 redo log 其实都不是 MySQL 数据库层面的日志，而是 InnoDB 存储引擎的日志。二者的作用联系紧密，事务的隔离性由锁来实现，原子性、一致性、持久性通过数据库的 redo log 或 redo log 来完成。redo log 又称为重做日志，用来保证事务的持久性，undo log 用来保证事务的原子性和 MVCC。

### 二进制日志（binlog）
binlog 是 没有 MySQL sever 层维护的一种二进制日志，与 innodb 引擎中的 redo/undo log 是完全不同的日志。其主要是用来记录对 MySQL 数据更新或潜在发生更新的 SQL 语句，并以 “事务”的形式保存在磁盘中。
#### 作用：
- 复制：MySQL 主从复制在 Master 端开启 binlog，Master 把它的二进制日志传递给 slaves 并回放来达到 master-slave 数据一致的目的
- 数据恢复：通过 mysqlbinlog 工具恢复数据
- 增量备份

#### 主从复制
复制是 MySQL 最重要的功能之一，MySQL 集群的高可用、负载均衡和读写分离都是基于复制来实现。复制步骤如下：

1. Master 将数据改变记录到二进制日志(binary log)中。
2. Slave 上面的 IO 进程连接上 Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。
3. Master 接收到来自 Slave 的 IO 进程的请求后，负责复制的 IO 进程会根据请求信息读取日志指定位置之后的日志信息，返回给 Slave 的 IO 进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到 Master 端的 binlog 文件的名称以及 binlog 的位置。
4. Slave 的 IO 进程接收到信息后，将接收到的日志内容依次添加到 Slave 端的 relaylog 文件的最末端，并将读取到的 Master 端的 binlog 的文件名和位置记录到 masterinfo 文件中，以便在下一次读取的时候能够清楚的告诉 Master 从某个 binlog 的哪个位置开始往后的日志内容
5. Slave 的 SQL 进程检测到 relaylog 中新增加了内容后，会马上解析 relaylog 的内容成为在 Master 端真实执行时候的那些可执行的内容，并在自身执行。

#### 知识点
- binlog 不会记录不修改数据的语句，比如Select或者Show
- binlog 会重写日志中的密码，保证不以纯文本的形式出现
- MySQL 8 之后的版本可以选择对 binlog 进行加密
- 具体的写入时间：在事务提交的时候，数据库会把 binlog cache 写入 binlog 文件中，但并没有执行fsync()操作，即只将文件内容写入到 OS 缓存中。随后根据配置判断是否执行 fsync。
- 删除时间：保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。

#### 内容：
- 逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。
- 但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。
- 在使用mysqlbinlog解析binlog之后一些都会真相大白。
- 因此可以基于binlog做到类似于oracle的闪回功能，其实都是依赖于binlog中的日志记录。

#### 什么时候产生：
- 事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中。
- 这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就开始逐步写入磁盘。
- 因此对于事务的提交，即便是较大的事务，提交（commit）都是很快的，但是在开启了bin_log的情况下，对于较大事务的提交，可能会变得比较慢一些。
- 这是因为binlog是在事务提交的时候一次性写入的造成的，这些可以通过测试验证。

#### 什么时候释放：
- binlog的默认是保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。

#### 对应的物理文件：
- 配置文件的路径为log_bin_basename，binlog日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。
- 对于每个binlog日志文件，通过一个统一的index文件来组织。

#### 其他：
- 二进制日志的作用之一是还原数据库的，这与redo log很类似，很多人混淆过，但是两者有本质的不同
- 作用不同：redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），虽然都有还原的意思，但是其保护数据的层次是不一样的。
- 内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句
- 另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。
- 恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog
- 关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。


#### 管理
有这几个常用的命令可以查看 binlog 的状态：
- binlog 的配置信息：show variables like '%log_bin%';
- binlog 的格式：show variables like 'binlog_format';
- 日志的文件列表：show binary logs;
- 当前日志的写入状态：show master status;
- 清空 binlog 日志：reset master;

#### 格式
binlog 日志有 Row、Statement、Mixed 三种格式。可以通过 my.cnf 配置文件及 set global binlog_format='ROW/STATEMENT/MIXED'进行修改，命令行 show variables like 'binlog_format' 命令查看 binglog 格式。

#### Row格式
Row 格式仅保存记录被修改细节，不记录 sql 语句上下文相关信息。新版本的 MySQL 默认是 Row 格式。

- 优点：能非常清晰的记录下每行数据的修改细节，不需要记录上下文相关信息，因此不会发生某些特定情况下的存储过程、函数或者触发器的调用触发无法被正确复制的问题，任何情况都可以被复制，且能加快从库重放日志的效率，保证从库数据的一致性
- 缺点：由于所有的执行的语句在日志中都将以每行记录的修改细节来记录，因此，可能会产生大量的日志内容，干扰内容也较多。比如一条 update 语句，如修改多条记录，则 binlog 中每一条修改都会有记录，这样造成 binlog 日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中，实际等于重建了表。


#### Statement格式
每一条会修改数据的 sql 都会记录在 binlog 中。

- 优点：
   - 只需要记录执行语句的细节和上下文环境，避免了记录每一行的变化，在一些修改记录较多的情况下相比 Row 格式能大大减少 binlog 日志量，节约 IO，提高性能。
   - 另外还可以用于实时的还原。
   - 主从版本可以不一样，从服务器版本可以比主服务器版本高。
- 缺点：
   - 为了保证 sql 语句能在 slave 上正确执行，必须记录上下文信息，以保证所有语句能在 slave 得到和在 master 端执行时候相同的结果。
   - 另外，主从复制时，存在部分函数（如 sleep）及存储过程在 slave 上会出现与 master 结果不一致的情况，而相比 Row 记录每一行的变化细节，绝不会发生这种不一致的情况。

#### Mixed格式
以上两种格式混合。

经过前面的对比，可以发现 Row 和 Statement 各有优势，如果可以根据 sql 语句取舍可能会有更好地性能和效果。Mixed 便是以上两种形式的结合。不过，新版本的 MySQL 对 Row 模式也做了优化，并不是所有的修改都会完全以 Row 形式来记录，像遇到表结构变更的时候就会以 Statement 模式来记录，如果 SQL 语句确实就是 update 或者 delete 等修改数据的语句，那么还是会记录所有行的变更；因此，现在一般使用 Row 即可。


### 三种日志总结
首先 InnoDB 完成一次更新操作的具体步骤：
- 开启事务
- 查询待更新的记录到内存，并加 X 锁
- 记录 undo log 到内存 buffer
- 记录 redo log 到内存 buffer
- 更改内存中的数据记录
- 提交事务，触发 redo log 刷盘
- 记录 bin log
- 事务结束

既然有两种日志，就会有一个写入的策略问题，这个问题也就引出了另一个概念——两阶段提交。所谓两阶段提交，其实就是将redo的提交拆分成了prepare和commit两个阶段，注意这里的commit不是commit语句，是一种状态。

当事务发起commit的时候，根据上面的描述，首先会将脏数据块写入redo log，但是此时还没有写binlog，因此阶段处于prepare阶段，只有当binlog完成了写操作之后，才会将redo log标记为commit，这个事务才算是真的完结了。

这时，我们来思考一个问题，如果写binlog的时候crash了，怎么办？因为redo log还是处于prepare状态，实际上事务没有真的commit，因此是会回滚的。




### 错误日志
错误日志记录着mysqld启动和停止,以及服务器在运行过程中发生的错误的相关信息。在默认情况下，系统记录错误日志的功能是关闭的，错误信息被输出到标准错误输出。

指定日志路径两种方法:
- 编辑my.cnf 写入 log-error=[path]
- 通过命令参数错误日志 mysqld_safe –user=mysql –log-error=[path] &

显示错误日志的命令
```sql
show variables like 'err';
```

### 普通查询日志 general query log 
记录了服务器接收到的每一个查询或是命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。也正因为mysql服务器需要不断地记录日志，开启General log会产生不小的系统开销。 因此，Mysql默认是把General log关闭的。

查看日志的存放方式
```sql
show variables like 'log_output';

set global log_output='table'； 　-- 设置日志结果会记录到名为gengera_log的表中，这表的默认引擎都是CSV

set global log_output='file';    -- 设置表数据存储到文件
set global general_log_file=’/tmp/general.log’; -- 设置general log的日志文件路径

set global general_log=on;  -- 开启general log： 
set global general_log=off;  --　关闭general log： 
```

### 慢查询日志 
慢日志记录执行时间过长和没有使用索引的查询语句，报错select、update、delete以及insert语句，慢日志只会记录执行成功的语句。

```sql
--  查看慢查询时间
show variables like "long_query_time"; -- 默认10s

-- 查看慢查询配置情况： 
show status like "%slow_queries%";

-- 查看慢查询日志路径： 
show variables like "%slow%";

-- 开启慢日志
 set global slow_query_log=1;

-- 查看已经开启
show variables like "slow_query_log";
```

## 主从复制
MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。

### MySQL复制原理
1. master服务器将数据的改变记录到二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；

2. slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件

3. 同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。

注:
- 从库会生成两个线程,一个I/O线程,一个SQL线程;
- 从库I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;
- 主库会生成一个log dump线程,用来给从库I/O线程传binlog;
- 从库地SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行;

注意事项:
1. master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。
2. slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。 
3. Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。 
4. Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本）
5. master和slave两节点间时间需同步

### 具体步骤：
1. 从库通过手工执行change master to 语句连接主库，提供了连接的用户一切条件（user 、password、port、ip），并且让从库知道，二进制日志的起点位置（file名 position 号）； start slave

2. 从库的IO线程和主库的dump线程建立连接。

3. 从库根据change master to 语句提供的file名和position号，IO线程向主库发起binlog的请求。

4. 主库dump线程根据从库的请求，将本地binlog以events的方式发给从库IO线程。

5. 从库IO线程接收binlog events，并存放到本地relay-log中，传送过来的信息，会记录到http://master.info中

6. 从库SQL线程应用relay-log，并且把应用过的记录到http://relay-log.info中，默认情况下，已经应用过的relay 会自动被清理purge。


### MySQL主从形式
- 一主一从
- 主主复制
- 一主多从
- 多主一从
- 联级复制: 将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。

### MySQL主从同步延时分析
mysql的主从复制都是单线程的操作，主库对所有DDL和DML产生的日志写进binlog，由于binlog是顺序写，所以效率很高，slave的sql thread线程将主库的DDL和DML操作事件在slave中重放。DML和DDL的IO操作是随机的，不是顺序，所以成本要高很多，另一方面，由于sql thread也是单线程的，当主库的并发较高时，产生的DML数量超过slave的SQL thread所能处理的速度，或者当slave中有大型query语句产生了锁等待，那么延时就产生了。

解决方案：
1. 业务的持久化层的实现采用分库架构，mysql服务可平行扩展，分散压力。

2. 单个库读写分离，一主多从，主写从读，分散压力。这样从库压力比主库高，保护主库。

3. 服务的基础架构在业务和mysql之间加入memcache或者redis的cache层。降低mysql的读压力。

4. 不同业务的mysql物理上放在不同机器，分散压力。

5. 使用比主库更好的硬件设备作为slave，mysql压力小，延迟自然会变小。

6. 使用更加强劲的硬件设备

参考资料:
- [mysql主从复制原理](https://zhuanlan.zhihu.com/p/96212530)
- [Mysql 主从复制](https://www.jianshu.com/p/faf0127f1cb2)


## 读写分离
MySQL读写分离基本原理是让master数据库处理写操作，slave数据库处理读操作。master将写操作的变更同步到各个slave节点。

MySQL读写分离能提高系统性能的原因在于：
- 物理服务器增加，机器处理能力提升。拿硬件换性能。
- 主从只负责各自的读和写，极大程度缓解X锁和S锁争用。
- slave可以配置myiasm引擎，提升查询性能以及节约系统开销。
- master直接写是并发的，slave通过主库发送来的binlog恢复数据是异步。
- slave可以单独设置一些参数来提升其读的性能。
- 增加冗余，提高可用性。

### MySQLProxy
下面使用MySQL官方提供的数据库代理层产品MySQLProxy搭建读写分离。
MySQLProxy实际上是在客户端请求与MySQLServer之间建立了一个连接池。所有客户端请求都是发向MySQLProxy，然后经由MySQLProxy进行相应的分析，判断出是读操作还是写操作，分发至对应的MySQLServer上。对于多节点Slave集群，也可以起做到负载均衡的效果。
参考资料:
- [MySQL读写分离介绍及搭建](https://segmentfault.com/a/1190000003716617)

### MaxScale
MaxScale 是Mysql 的兄弟公司 MariaDB 开发的，现在已经发展得非常成熟。MaxScale 是插件式结构，允许用户开发适合自己的插件。
MaxScale 目前提供的插件功能分为5类
参考资料:
- [轻松实现MySQL读写分离](https://www.jianshu.com/p/70d94a8f6491)

## 分库分表
### 数据库瓶颈
不管是IO瓶颈，还是CPU瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载活跃连接数的阈值。在业务Service来看就是，可用数据库连接少甚至无连接可用。接下来就可以想象了吧（并发量、吞吐量、崩溃）。
1. IO瓶颈
    - 第一种：磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -> 分库和垂直分表。
    - 第二种：网络IO瓶颈，请求的数据太多，网络带宽不够 -> 分库。

2. CPU瓶颈
    - 第一种：SQL问题，如SQL中包含join，group by，order by，非索引字段条件查询等，增加CPU运算的操作 -> SQL优化，建立合适的索引，在业务Service层进行业务计算。
    - 第二种：单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -> 水平分表。

### 分库分表方式
#### 水平分库
概念：以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中。

结果：
- 每个库的结构都一样；
- 每个库的数据都不一样，没有交集；
- 所有库的并集是全量数据；
  
场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。

分析：库多了，io和cpu的压力自然可以成倍缓解。

#### 水平分表
概念：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中。

结果：
- 每个表的结构都一样；
- 每个表的数据都不一样，没有交集；
- 所有表的并集是全量数据；

场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。

分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。

#### 垂直分库
概念：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。

结果：
- 每个库的结构都不一样；
- 每个库的数据也不一样，没有交集；
- 所有库的并集是全量数据；

场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。

分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。

#### 垂直分表
概念：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。

结果：
- 每个表的结构都不一样；
- 每个表的数据也不一样，一般来说，每个表的字段至少有一列交集，一般是主键，用于关联数据；
- 所有表的并集是全量数据；

场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。

分析：可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。

### 分库分表工具
- sharding-sphere：jar，前身是sharding-jdbc；
- TDDL：jar，Taobao Distribute Data Layer；
- Mycat：中间件。

### 分库分表步骤
根据容量（当前容量和增长量）评估分库或分表个数 -> 选key（均匀）-> 分表规则（hash或range等）-> 执行（一般双写）-> 扩容问题（尽量减少数据的移动）。

### 分库分表问题
基于水平分库分表，拆分策略为常用的hash法。
1. 非partition key的查询问题
    - 冗余法
    - NoSQL法
2. 非partition key跨库跨表分页查询问题
    - 注：用**NoSQL法**解决（ES等）。
3. 扩容问题
    - 扩容是成倍的。

水平扩容库（升级从库法）

### 分库分表总结
1. 分库分表，首先得知道瓶颈在哪里，然后才能合理地拆分（分库还是分表？水平还是垂直？分几个？）。且不可为了分库分表而拆分。

2. 选key很重要，既要考虑到拆分均匀，也要考虑到非partition key的查询。

3. 只要能满足需求，拆分规则越简单越好。

### 分库分表示例
[Gihub地址](https://github.com/littlecharacter4s/study-sharding)

参考资料:[MySQL：互联网公司常用分库分表方案汇总！](https://zhuanlan.zhihu.com/p/137368446)


#### 那么分库分表多少合适呢?

经测试在单表1000万条记录一下,写入读取性能是比较好的. 这样在留点buffer,那么单表全是数据字型的保持在800万条记录以下, 有字符型的单表保持在500万以下。

如果按 100库100表来规划,如用户业务:

500万*100*100 = 50000000万 = 5000亿记录。

心里有一个数了，按业务做规划还是比较容易的。